# High-level integration problems Section 1: Justifications for Persistent Storage (Generic Case)

There have been a number of different discussion threads with regards to making a persistent storage mechanism available 
to integration flows.  These discussions have used various vocabularies to discuss these ideas such as DataHub, Key
Value Storage, S2 component etc.  The goal for this section is to establish a common understanding of the problems 
that exist so that appropreate steps can be taken in the future.  

## Case 1: ID Linking

### Problem Synopsis:

The business need is to syncronize mutable data between two or more systems where the individual data records lack a 
natural primary key that is immutable.  
When a data record is changed or created in one system, we need to be able to identify the corresponding record in 
the other system to change or update. 

### What the Non Persistent Storage Looks Like (And the Corresponding Problems):

There is peer-to-peer tracking of IDs by adding a column/field on each system to keep track of the ID generated by
the other system.  The problems with this are:

- Not every system can be configured to add columns
- There should be uniqueness contraints on these columns
- Querying by these columns needs to be fast
- End users should not casually be able to inadvertenly edit these fields
- Both systems need to have the `Lookup Object By Unique Criteria` implemented when it wouldn’t be otherwise required
- Syncronizing objects which that have a 0..N relationship with other objects becomes very difficult.
- This solution becomes very complex if there are more than two systems that need to be integrated simultaneously, 
especially if a system is not naturally the center.

### How It Can Be Solved with Persistent Storage (MongoDB example):

Given a list of N systems and M object types: 

- Create one collection per object type
- For each object that is syncronized, there will be an object in the corresponding collection.  As IDs are assigned
to objects, add a field for that ID
- There should be an index per system per object type that makes lookups fast and enforces uniqueness.

## Case 2: Delta Detection

### Problem(s) Synopsis:

- You need to learn about new records, updates to records and deletions of records from a system which does not
provide timebased change tracking or webhooks.
- You want to watch and act on updates only when certain fields on an object change.

### What the Non Persistent Storage Looks Like (And the Corresponding Problems):

- Pull all records (say every 24 hours) and process them all regardless.
- Don’t propogate deletes.

### How It Can Be Solved with Persistent Storage (MongoDB example):

For every object type that wants to be watched:

- Create a collection
- As objects are extracted, look in the DB for a record with the same ID.
  - If none is found:
    - compute a hash of the object (or hash per relevant field)
    - write the hash, object ID and current timestamp to the DB
    - pass the object forward
  - If a record is found:
    - Hash the current object (or hash per relevant field)
    - Update the timestamp in the DB to now.
    - Compare the current hash to the old hash
    - If the hashes match, don’t propogate further
    - If the hashes don’t match, store the new hash and propogate the object
- Have a second flow watch for deletes.  
  - This flow will be scheduled to wake up and look in the DB for records that haven’t been seen since some threshold.  
  - In this case, consider the object deleted.
  - Propogate the deletion and then remove the record from the DB.

## Case 3: Batching

### Problem(s) Synopsis:

- Some APIs do not allow unlimited API calls or charge money for API calls above a certain threshold but allow batches.
- Some APIs have batch APIs which are faster than calling the APIs one by one.

Examples: 

- Amazon Marketplace limits API calls to 30 per hour but allows effectively unlimited number of calls per batch. 
Syncing a catalog with 90 000 products one at a time would take more than 4 months.
- Salesforce Force.com allows 15 000 calls per 24 hours (more are available for purchase and non-publically disclosed prices.
Their are bulk API calls that allow 10 000 per batch and up to 10 000 records per batch.  Given 1 million records,
batch APIs allow an import over one day while non-batch would require more than 2 months.

Needs:

- No batch should exceed the maximum allowed size
- No record should wait an unreasonably long time for a full batch

### What the Non Persistent Storage Looks Like (And the Corresponding Problems):

- Use one-by-one and deal with the costs.

or

- Build a batching mechanism that is similar to the one used in CSV write where container crashes loose data and you
can’t solve the `No record should wait an unreasonably long time for a full batch` requirement or have larger
time to fill batches

### How It Can Be Solved with Persistent Storage (MongoDB example):

A batch has the following propeties:

- Max time between batches
- Max size in records
- Max size in bytes/chars

The MongoDB has two collections:

- One for batches
- One for items

On recieving an item in the batch action:

- Create an open batch if one does not exist
- Add the item to the open batch
- Check if the batch should be closed

There is then another trigger to read from batches:

- Poll regularly for batches that should be closed and shipped
- For each batch to ship, read all items into an attachment and publish a message with the attachment

## Case 4: Scheduled Jobs (Special Case Is for Async Request Tracking)

### Problem(s) Synopsis:

- Some APIs are async:  You submit a request and then get a link you can poll to see if the results are ready
- There are client specific cases where you want to schedule cases to change in the future

### How It Can Be Solved with Persistent Storage (MongoDB example):

- A mongoDB has a table full of timestamps and info needed to execute the job


## Case 5: Combining (Both different branches and the message → array → message)

### Problem(s) Synopsis:

- The platform has allows (in a non-supported way) for flow branches to converge.  We may want to allow a message to go
down multiple paths in parallel before being reconstruction.
- Sometimes you have a message with an array.  You want to do some number of steps for each array entry and then once 
done, reconstruct the array. 

## How It Can Be Solved with Persistent Storage (MongoDB example):

- A mongoDB has a table full of records with needed info

## Case 6: Cache

### Problem(s) Synopsis:

- You have an API which is slow or can only be querried a limited number of times so you want to mirror the data in the system into an external system.

### What the Non Persistent Storage Looks Like (And the Corresponding Problems):

- Realtime flow where you mirror the data into memory

### How It Can Be Solved with Persistent Storage (MongoDB example):

- A mongoDB has a collection with the mirrored data and a flow that ensures the mirror is up to date.


## Case 7: Data Hub

### Problem(s) Synopsis:

- You want to syncronize data between multiple systems in a simple hub and spoke architecture with no system is well suited to be a hub

### What the Non Persistent Storage Looks Like (And the Corresponding Problems):

- Build complex set of flows

### How It Can Be Solved with Persistent Storage (MongoDB example):

- A mongoDB is that hub.

## Resources
- Example Component Completeness Matrixes:
  - SalesForce: https://github.com/elasticio/salesforce-component
- Link to Standardized Component Behaviors: https://github.com/openintegrationhub/Connectors/blob/master/Adapters/AdapterBehaviorStandardization/StandardizedActionsAndTriggers.md
